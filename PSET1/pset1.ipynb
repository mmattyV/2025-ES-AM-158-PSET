{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c41cb8",
   "metadata": {},
   "source": [
    "# Problem Set 1 — Four Problems in Dynamic Programming\n",
    "\n",
    "*Markov Decision Processes & DP Methods*\n",
    "\n",
    "**Due: September 26 (Friday), 11:59pm**\n",
    "\n",
    "### Problem 1 — Inscribed Polygon of Maximum Perimeter (Pen & Paper)\n",
    "TODO list:\n",
    "- (a) show Q-value function $Q_{N-1}$ (6pt)\n",
    "- (b) show convexity (6pt)\n",
    "- (c) show optimal control signal $u_{N-1}$ (6pt)\n",
    "- (d) induction to any $k$-th step Q-function $Q_k$ (6pt)\n",
    "- (e) show all optimal control signal $u_k$ (6pt)\n",
    "\n",
    "Bonus:\n",
    "- (f) show convexity (5pt)\n",
    "- (g)(coding) solve the problem using optimization (5pt)\n",
    "### Problem 2 — Proof of convergence of value iteration (Pen & Paper)\n",
    "TODO list:\n",
    "- 2.1 contraction of bellman operator (5pt)\n",
    "- 2.2 linear convergence (5pt)\n",
    "- 2.3 stoping criteria (5pt)\n",
    "- 2.4 iteration bound (5pt)\n",
    "### Problem 3 — Cliffwalk (coding)\n",
    "TODO list:\n",
    "- 3.2 fill in code for policy evaluation (10pt)\n",
    "- 3.3 fill in code for policy iteration (10pt)\n",
    "- 3.4 fill in code for value iteration (10pt)\n",
    "\n",
    "### Problem 4 — Matrix–Vector Representation of DP\n",
    "TODO list:\n",
    "- 4.1 build the transition matrix $P$ (5pt)\n",
    "- 4.2 write bellman equation as matrix form (5pt)\n",
    "- 4.3 solve the matrix equation by fix-point iteration (10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999f44b8",
   "metadata": {},
   "source": [
    "## 1. Inscribed Polygon of Maximal Perimeter (Pen and Paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bccd1ea",
   "metadata": {},
   "source": [
    "In lectures, we have seen how dynamic programming (DP) can compute optimal value functions and optimal policies for finite-horizon MDPs with discrete state space and action space (i.e., the tabular case).\n",
    "\n",
    "In this exercise, we will see that DP can also solve an optimal control problem with continuous state space and action space.\n",
    "This problem is a geometry problem where we try to find the $N$-side polygon inscribed inside a circle with maximum perimeter. We will walk you through the key steps of formulating and solving the problem, while leaving a few mathematical details for you to fill in.\n",
    "\n",
    "Given a circle with radius $1$, we can randomly choose $N$ distinct points on the circle to form a polygon with $N$ vertices and sides, as shown in Fig. 1 with $N=3,4,5$.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/ComputationalRobotics/2025-ES-AM-158-PSET/main/PSET1/polygon-inside-circle.png\" width=\"600\" alt=\"Inscribed polygon\">\n",
    "  <figcaption style=\"color:#6a737d; font-style:italic;\">\n",
    "    Figure 1. Polygons inscribed inside a circle\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "Once the $N$ points are chosen, the $N$-polygon will have a perimeter, i.e., the sum of the lengths of its edges.\n",
    "\n",
    "What is the configuration of the $N$ points such that the resulting $N$-polygon has the maximum perimeter? We claim that the answer is when the $N$-polygon has edges of equal lengths, or in other words, when the $N$ points are placed on the circle evenly.\n",
    "\n",
    "Let us use dynamic programming to prove the claim.\n",
    "\n",
    "To use dynamic programming, we need to define a dynamical system and a reward function.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/ComputationalRobotics/2025-ES-AM-158-PSET/main/PSET1/sequential-placement-N-point.png\" width=\"360\" alt=\"Inscribed polygon\">\n",
    "  <figcaption style=\"color:#6a737d; font-style:italic;\">\n",
    "    Figure 2. Sequential placement of N points on the circle.\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "**Dynamical system.**\n",
    "\n",
    "\n",
    "We will use $\\{x_1, \\ldots, x_N\\}$ to denote the angular positions of the $N$ points to be placed on the circle (with slight abuse of notation, we will call each of those points $x_k$ as well). In particular, as shown in Fig. 2, let us use $x_k$ to denote the angle between the line $O — x_k$ and the vertical line (O is the center of the circle), with zero angle starting at 12 o’clock and clockwise being positive. Without loss of generality, we assume $x_1 = 0$. (if $x_1$ is nonzero, we can always rotate the entire circle so that $x_1 = 0$).\n",
    "\n",
    "After the $k$-th point is placed, we can “control” where the next point $x_{k+1}$ will be, by deciding the incremental angle between $x_{k+1}$ and $x_k$, denoted as $u_k > 0$ in Fig. 2. This is simply saying the dynamics is\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k + u_k, \\quad k=1,\\ldots,N-1, \\quad x_1 = 0.\n",
    "$$\n",
    "\n",
    "Notice here we did not use an MDP to formulate this problem because the dynamics is deterministic. In the MDP language, this would correspond to, at time step $k$, if the agent takes action $u_k$ at state $x_k$, then the probability of transitioning to state $x_k + u_k$ at time $k+1$ is $1$, and the probability of transitioning to other states is zero.\n",
    "\n",
    "\n",
    "**Reward.**\n",
    "\n",
    "\n",
    "The perimeter of the $N$-polygon is therefore\n",
    "\n",
    "$$\n",
    "g_N(x_N) + \\sum_{k=1}^{N-1} g_k(x_k,u_k),\n",
    "$$\n",
    "\n",
    "with the terminal reward\n",
    "\n",
    "$$\n",
    "g_N(x_N) = 2 \\sin\\left(\\frac{2\\pi-x_N}{2}\\right),\n",
    "$$\n",
    "\n",
    "the distance between $x_N$ and $x_1$ (see Fig. 2), and the running reward\n",
    "\n",
    "$$\n",
    "g_k(x_k,u_k) = 2 \\sin\\left(\\frac{u_k}{2}\\right).\n",
    "$$\n",
    "\n",
    "**Dynamic programming.**\n",
    "\n",
    "We are now ready to invoke dynamic programming. Recall in lectures the key steps of DP are first to initialize the optimal value functions at the terminal time $k=N$, and then perform backward recursion to compute the optimal value functions at time $k=N-1,\\dots,1$.\n",
    "\n",
    "We start by setting\n",
    "\n",
    "$$\n",
    "V_N(x_N) = g_N(x_N) = 2 \\sin\\left(\\frac{2\\pi-x_N}{2}\\right).\n",
    "$$\n",
    "\n",
    "Unlike in lectures where we initialized the terminal value functions as all zero, here we initialize the terminal value functions as $g_N(x_N)$ because there is a \"terminal-state\" reward.\n",
    "\n",
    "We then compute $V_{N-1}(x_{N-1})$ as\n",
    "\n",
    "$$\n",
    "V_{N-1}(x_{N-1})\n",
    "= \\max_{0 < u_{N-1} < 2\\pi-x_{N-1}}\n",
    "\\left\\{\n",
    "  \\underbrace{ 2 \\sin\\left(\\tfrac{u_{N-1}}{2}\\right) + V_N(x_{N-1} + u_{N-1}) }_{Q_{N-1}(x_{N-1}, u_{N-1})}\n",
    "\\right\\},\n",
    "\\tag{9.1}\n",
    "$$\n",
    "\n",
    "where $u_{N-1} < 2\\pi-x_{N-1}$ because we do not want $x_N$ to cross $2\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0764d9",
   "metadata": {},
   "source": [
    "\n",
    "**a**. Show that\n",
    "\n",
    "$$\n",
    "Q_{N-1}(x_{N-1}, u_{N-1})\n",
    "= 2 \\sin\\left(\\tfrac{u_{N-1}}{2}\\right)\n",
    "+ 2 \\sin\\left(\\tfrac{2\\pi-x_{N-1}-u_{N-1}}{2}\\right),\n",
    "$$\n",
    "\n",
    "and thus\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Q_{N-1}(x_{N-1}, u_{N-1})}{\\partial u_{N-1}}\n",
    "= \\cos\\left(\\tfrac{u_{N-1}}{2}\\right)\n",
    "- \\cos\\left(\\tfrac{2\\pi-x_{N-1}-u_{N-1}}{2}\\right).\n",
    "$$\n",
    "\n",
    "**(TODO) ANSWER:**\n",
    "From the definition above, we get given that:\n",
    "$$Q_{N-1}(x_{N-1}, u_{N-1}) = 2\\sin(\\frac{u_N-1}{2}) + V_N(x_{N-1}+u_{N-1})$$\n",
    "\n",
    "Using the fact that:\n",
    "$$V_N(x_N) = g_N(x_N) = 2 \\sin\\left(\\frac{2\\pi-x_N}{2}\\right)$$\n",
    "and substituting in $x_{N-1}+u_{N-1}$, get that:\n",
    "$$\n",
    "\\boxed{Q_{N-1}(x_{N-1}, u_{N-1})\n",
    "= 2 \\sin\\left(\\tfrac{u_{N-1}}{2}\\right)\n",
    "+ 2 \\sin\\left(\\tfrac{2\\pi-x_{N-1}-u_{N-1}}{2}\\right),}\n",
    "$$\n",
    "\n",
    "Applying the chain rule, we get that:\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial Q_{N-1}(x_{N-1}, u_{N-1})}{\\partial u_{N-1}}\n",
    "= \\cos\\left(\\tfrac{u_{N-1}}{2}\\right)\n",
    "- \\cos\\left(\\tfrac{2\\pi-x_{N-1}-u_{N-1}}{2}\\right).}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a666ead0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**b**. Show that $Q_{N-1}(x_{N-1}, u_{N-1})$ is concave (i.e., $-Q_{N-1}(x_{N-1}, u_{N-1})$ is convex) in $u_{N-1}$ for every $x_{N-1} \\in (0, \\pi)$ and $u_{N-1} \\in (0, 2\\pi-x_{N-1})$.\n",
    "(Hint: compute the second derivative of $Q_{N-1}(x_{N-1}, u_{N-1})$ with respect to $u_{N-1}$ and verify it is positive definite)\n",
    "\n",
    "**(TODO) ANSWER:**\n",
    "Using the chain rule again, we get that:\n",
    "$$\n",
    "\\frac{\\partial^2 Q_{N-1}(x_{N-1}, u_{N-1})}{\\partial u_{N-1}^2}\n",
    "= -\\frac{1}{2}\\sin\\!\\left(\\frac{u_{N-1}}{2}\\right)\n",
    "-\\frac{1}{2}\\sin\\!\\left(\\frac{2\\pi-x_{N-1}-u_{N-1}}{2}\\right).\n",
    "$$\n",
    "However, we know that $u_{N-1} \\in (0, 2\\pi-x_{N-1})$, so the first term is always less than $0$ because $\\sin$ of a number in $(0,\\pi)$ is positive (here $0<\\tfrac{u_{N-1}}{2}<\\pi$). For the second term, since $x_{N-1}\\in(0,\\pi)$ and $u_{N-1}\\in(0,2\\pi-x_{N-1})$, we have\n",
    "$$\n",
    "0<\\frac{2\\pi-x_{N-1}-u_{N-1}}{2}<\\pi,\n",
    "$$\n",
    "so $\\sin$ is also positive there. Then, since the first term is negative and we are subtracting a positive number in the second term, the entire second derivative is strictly less than $0$. Thus, $Q_{N-1}(x_{N-1}, u_{N-1})$ is concave in $u_{N-1}$ on $(0,2\\pi-x_{N-1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71bd6ca",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**c**. With a and b, show that the optimal $u_{N-1}$ that solves (9.1) is\n",
    "\n",
    "$$\n",
    "u_{N-1}^\\star = \\frac{2\\pi-x_{N-1}}{2},\n",
    "$$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$\n",
    "J_{N-1}(x_{N-1}) = 4 \\sin\\left(\\tfrac{2\\pi-x_{N-1}}{4}\\right).\n",
    "$$\n",
    "\n",
    "(Hint: the point at which a concave function’s gradient vanishes must be the unique maximizer of that function.)\n",
    "\n",
    "**(TODO) ANSWER:**\n",
    "Given the hint, if plugging in $u_{N-1}^\\star = \\frac{2\\pi-x_{N-1}}{2}$ for the second derivative makes it vanish, then it must be the unique maximizer of the underlying function. If we plug $u_{N-1}^\\star$ into the second derivative and simplifying, we get:\n",
    "$$\n",
    "\\frac{\\partial^2 Q_{N-1}(x_{N-1}, u_{N-1}^\\star)}{\\partial u_{N-1}^2}\n",
    "= -\\frac{1}{2}\\sin\\!\\left(\\frac{1}{2}\\pi - \\frac{x_{N-1}}{4}\\right)\n",
    "-\\frac{1}{2}\\sin\\!\\left(\\frac{1}{2}\\pi - \\frac{x_{N-1}}{4}\\right)\n",
    "=0.\n",
    "$$\n",
    "Thus, $u_{N-1}^\\star$ is indeed optimal. Then,\n",
    "$$\n",
    "J_{N-1}(x_{N-1}) = Q_{N-1}(x_{N-1},u_{N-1}^\\star) = 2\\sin(\\frac{2\\pi-x_{N-1}}{4})+2\\sin(\\frac{2\\pi-x_{N-1}}{4}) = 4 \\sin\\left(\\tfrac{2\\pi-x_{N-1}}{4}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c9458",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**d**. Now use induction to show that the $k$-th step dynamic programming\n",
    "\n",
    "$$\n",
    "J_k(x_k)\n",
    "= \\max_{0 < u_k < 2\\pi — x_k}\n",
    "\\left\\{ 2 \\sin\\left(\\tfrac{u_k}{2}\\right) + J_{k+1}(x_k + u_k) \\right\\}\n",
    "$$\n",
    "\n",
    "admits an optimal control\n",
    "\n",
    "$$\n",
    "u_k^\\star = \\frac{2\\pi-x_k}{N-k + 1},\n",
    "$$\n",
    "\n",
    "and optimal cost-to-go\n",
    "\n",
    "$$\n",
    "J_k(x_k) = 2 (N-k + 1) \\, \\sin\\!\\left( \\frac{2\\pi-x_k}{2 (N-k + 1)} \\right).\n",
    "$$\n",
    "\n",
    "**(TODO) ANSWER:**\n",
    "To prove that the optimal control and cost-to-go is valid, we already verified that the base case for $N-1$ is true. Our induction hypothesis is that for some $k+1$:\n",
    "$$J_{k+1}(x_{k+1})=2m\\sin(\\frac{2\\pi-x_{k+1}}{2m})$$\n",
    "where $m=N-k$. We now form the $Q$-function at stage $k$ using $x_{k+1}=x_k+u_k$:\n",
    "$$Q_k(x_k,u_k)=2\\sin(\\frac{u_k}{2})+2m\\sin(\\frac{2\\pi-x_k-u_k}{2m})$$\n",
    "where $0< u_k< 2\\pi-x_k$. We then check the concavity of this function by finding its second derivative:\n",
    "$$\\frac{\\partial^2 Q_k}{\\partial u_k^2}=-\\frac{1}{2}\\sin(\\frac{u_k}{2})-\\frac{1}{2m}\\sin(\\frac{2\\pi-x_k-u_k}{2m})$$ \n",
    "Like in part b, since the both angles lie in $(0,\\pi)$, we get that $Q_k$ is strictly concave in the feasible intervals. Now, we check the first derivative to see where the maximum of $Q_k$ is:\n",
    "\n",
    "$$\\frac{\\partial Q_k}{\\partial u_k}=\\cos(\\frac{u_k}{2})-\\cos(\\frac{2\\pi-x_k-u_k}{2m})=0$$\n",
    "Again. since both angles lie in $(0,\\pi)$ and because $\\cos$ is injective on that interval, $Q_k$ is at its max value when:\n",
    "$$\\frac{u_k}{2}=\\frac{2\\pi-x_k-u_k}{2m}$$\n",
    "\n",
    "Let $\\theta=\\frac{u_k}{2}=\\frac{2\\pi-x_k-u_k}{2m}$. Then, substituting into $Q_k$, we get that:\n",
    "$$J_k(x_k)=Q_k(x_k,u_k^\\star)=2\\sin(\\theta)+2m\\sin(\\theta)$$\n",
    "We now solve for $\\theta$ in terms of $x_k$ and $m$:\n",
    "$$\\frac{u_k}{2}=\\frac{2\\pi-x_k-u_k}{2m}\\to u_k(m+1)=2\\pi-x_k\\to u_k=\\frac{2\\pi-x_k}{m+1}\\to\\theta=\\frac{2\\pi-x_k}{2(m+1)}$$\n",
    "Then, we finally get that:\n",
    "$$J_k(x_k)=2 (N-k + 1) \\, \\sin\\!\\left( \\frac{2\\pi-x_k}{2 (N-k + 1)} \\right)$$\n",
    "after subbing in $m$ and $\\theta$. This also proves $u_k^\\star = \\frac{2\\pi-x_k}{N-k + 1}$. Thus, we have proven the theorem by induction. QED."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8ec40",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**e**. Starting from $x_1 = 0$, what is the optimal sequence of controls?\n",
    "\n",
    "Hopefully now you see why my original claim is true!\n",
    "\n",
    "**(TODO) ANSWER:**\n",
    "We just want to follow:\n",
    "$$u_k^\\star = \\frac{2\\pi-x_k}{N-k + 1}$$\n",
    "that we found above. From the original claim, we get that for all $k=1,\\dots,N-1$:\n",
    "$$u_k^\\star=\\frac{2\\pi}{N}\\text{ and } x_k=\\frac{2\\pi(k-1)}{N}$$\n",
    "We want to do a proof by induction. We start in the base case where $k=1:u_1^\\star=\\frac{2\\pi}{N}\\to x_2=\\frac{2\\pi}{N}$. Our inductive step is then:\n",
    "$$x_k=\\frac{2\\pi(k-1)}{N}$$\n",
    "Since we have:\n",
    "$$u_k^\\star=\\frac{2\\pi-\\frac{2\\pi(k-1)}{N}}{N-k+1}=\\frac{2\\pi}{N}, x_{k+1}=x_k+u_k^\\star=\\frac{2\\pi k}{N}$$\n",
    "Thus, the optimal control sequence is:\n",
    "$$u_1^\\star=u_2^\\star=\\dots=u_{N-1}^\\star=\\frac{2\\pi}{N} \\text{ and } x_k=\\frac{2\\pi(k-1)}{N}.$$\n",
    "\n",
    "QED.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dfd1d1",
   "metadata": {},
   "source": [
    "### Bonus:\n",
    "We are not yet done for this exercise. Since you have probably already spent quite some time on this exercise, I will leave the rest of the exercise a bonus. In case you found this simple geometric problem interesting, you should keep reading as we will use numerical techniques to prove the same claim.\n",
    "\n",
    "In Fig. 2, by denoting\n",
    "\n",
    "$$\n",
    "u_N = 2\\pi - x_N = 2\\pi - (u_1 + \\cdots + u_{N-1})\n",
    "$$\n",
    "\n",
    "as the angle between the line $O — x_N$ and the line $O — x_1$, it is not hard to observe that the perimeter of the $N$-polygon is\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^N 2 \\sin\\!\\left(\\tfrac{u_k}{2}\\right).\n",
    "$$\n",
    "\n",
    "Consequently, to maximize the perimeter, we can formulate the following optimization\n",
    "\n",
    "$$\n",
    "\\max_{u_1,\\ldots,u_N} \\;\\; \\sum_{k=1}^N 2 \\sin\\!\\left(\\tfrac{u_k}{2}\\right)\n",
    "$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\n",
    "u_k > 0, \\; k = 1, \\ldots, N, \\\\\n",
    "u_1 + \\cdots + u_N = 2\\pi\n",
    "\\tag{9.2}\n",
    "$$\n",
    "\n",
    "where $u_k$ can be seen as the angle spanned by the line $x_k — x_{k+1}$ with respect to the center $O$ so that they are positive and sum up to $2\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21f4bb8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**f**. Show that the optimization (9.2) is convex.\n",
    "(Hint: first show the feasible set is convex, and then show the objective function is concave over the feasible set.)\n",
    "\n",
    "**(TODO) ANSWER:**\n",
    "\n",
    "Now that we have shown (9.2) is a convex optimization problem, we know that pretty much any numerical algorithm will guarantee convergence to the globally optimal solution.\n",
    "\n",
    "There are many numerical algorithms that can compute optimal solutions of an optimization problem (Nocedal and Wright 1999). Python provides a nice interface, `scipy.optimize`, to many such algorithms, and let us use `scipy.optimize` to solve (9.2) so we can numerically prove our claim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d86a56",
   "metadata": {},
   "source": [
    "\n",
    "**g**. We have provided most of the code necessary for solving (9.2) below. Please fill in the definition of the function `perimeter(u)`, and then run the code. Show your results for $N = 3, 10, 100$. Do the solutions obtained from Python verify our claim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce6c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------- Parameters --------\n",
    "N = 10  # Number of points\n",
    "\n",
    "# -------- Objective: polygon perimeter on the unit circle (edge length = 2*sin(u/2)) --------\n",
    "def perimeter(u):\n",
    "    ##############################\n",
    "    # TODO BLOCK\n",
    "    ##############################\n",
    "\n",
    "def neg_perimeter(u):\n",
    "# SciPy minimizes; negate to perform maximization\n",
    "    return -perimeter(u)\n",
    "\n",
    "# -------- Constraints & initialization --------\n",
    "# Linear equality: sum(a) = 2π\n",
    "eq_cons = {'type': 'eq', 'fun': lambda u: np.sum(u) - 2.0 * np.pi}\n",
    "\n",
    "# Bounds: u_i ∈ [0, 2π] (upper bound helps numerics)\n",
    "bounds = [(0.0, 2.0 * np.pi)] * N\n",
    "\n",
    "# Initial guess: positive random vector normalized to 2π\n",
    "rng = np.random.default_rng(0)\n",
    "u0 = rng.random(N)\n",
    "u0 = u0 / u0.sum() * 2.0 * np.pi\n",
    "\n",
    "# -------- Solve (SLSQP) --------\n",
    "res = minimize(\n",
    "    neg_perimeter, u0,\n",
    "    method='SLSQP',\n",
    "    bounds=bounds,\n",
    "    constraints=[eq_cons],\n",
    "    options={'maxiter': 2000, 'ftol': 1e-12, 'disp': True}\n",
    ")\n",
    "\n",
    "uopt = res.x\n",
    "print(\"Success:\", res.success, \"| message:\", res.message)\n",
    "print(\"Perimeter =\", perimeter(uopt))\n",
    "\n",
    "# -------- Recover vertex angles x by cumulative sum (x[0]=0; others accumulate preceding gaps) --------\n",
    "x = np.zeros(N)\n",
    "x[1:] = np.cumsum(uopt[:-1])\n",
    "\n",
    "# -------- Plot --------\n",
    "fig, ax = plt.subplots()\n",
    "# Draw unit circle\n",
    "circle = plt.Circle((0, 0), 1.0, fill=False)\n",
    "ax.add_patch(circle)\n",
    "\n",
    "# Scatter vertices\n",
    "ax.scatter(np.cos(x), np.sin(x), s=40, label=\"points\")\n",
    "\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_xlim(-1.1, 1.1)\n",
    "ax.set_ylim(-1.1, 1.1)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(f\"N={N}, perimeter={perimeter(uopt):.6f}\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a181bad5",
   "metadata": {},
   "source": [
    "## 2. Convergence proof of Value iteration\n",
    "\n",
    "Let the Bellman optimality operator be\n",
    "$$\n",
    "(T^\\star V)(s)=\\max_a\\Big[\\,R(s,a)+\\gamma\\sum_{s'}P(s'|s,a)V(s')\\,\\Big],\n",
    "\\qquad \\gamma\\in[0,1).\n",
    "$$\n",
    "Let $V^\\star$ denote the optimal value function, i.e., $V^\\star=T^\\star V^\\star$.\n",
    "Value iteration is $V_{k+1}=T^\\star V_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b1668",
   "metadata": {},
   "source": [
    "### 2.1 Contraction\n",
    "\n",
    "We first prove the operator is a $\\gamma$-**contraction** in the sup norm, i.e.\n",
    "$$\n",
    "||V_{k+1}-V^\\star||_\\infty \\leq \\gamma ||V_k-V^\\star||_\\infty\n",
    "$$\n",
    "\n",
    "**(TODO) Answer:**\n",
    "Using the Bellman optimality operator, we have that:\n",
    "$$||V_{k+1}-V^\\star||_\\infty=||T^\\pi V_k-T^\\pi V^\\star||_\\infty$$\n",
    "Observe that for any $s\\in S$, we have:\n",
    "$$|(T^\\pi V_k)(s)-(T^\\pi V^\\star)(s)=|\\sum_a\\pi(a|s)\\gamma\\sum_{s'}P(s'|s,a)(V_k(s')-V^\\star(s'))|$$\n",
    "$$\\leq\\gamma\\sum_a\\pi(a|s)\\sum_{s'}P(s'|s,a)|V_k(s')-V^\\star(s')|$$\n",
    "$$\\leq\\gamma||V_k-V^\\star||_\\infty\\sum_a\\pi(a|s)\\sum_{s'}P(s'|s,a)$$\n",
    "$$=\\gamma||V_k-V^\\star||_\\infty$$\n",
    "\n",
    "Since we have $||V_{k+1}-V^\\star||_\\infty=||T^\\pi V_k-T^\\pi V^\\star||_\\infty$, taking the max over $s$ gives:\n",
    "$$||V_{k+1}-V^\\star||_\\infty \\leq \\gamma ||V_k-V^\\star||_\\infty$$\n",
    "QED."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b835d5",
   "metadata": {},
   "source": [
    "### 2.2 linear convergence\n",
    "Next we prove the convergence is actually **linear**, i.e.\n",
    "$$\n",
    "\\|V_k-V^\\star\\|_\\infty \\leq \\gamma^k \\|V_0-V^\\star\\|_\\infty\n",
    "$$\n",
    "\n",
    "**(TODO) Answer:**\n",
    "We will perform this proof by induction. Let $V_{k+1}=T^* V_k$ and $V^*=T^* V^*$. By the $\\gamma$-contraction of $T^*$ (from 2.1), for any $U,W$ we have\n",
    "$$\n",
    "||T^* U - T^* W||_\\infty \\le \\gamma \\, ||U - W||_\\infty .\n",
    "$$\n",
    "\n",
    "**Base case ($k=1$).**\n",
    "$$\n",
    "||V_1 - V^*||_\\infty\n",
    "= ||T^* V_0 - T^* V^*||_\\infty\n",
    "\\le \\gamma \\, ||V_0 - V^*||_\\infty\n",
    "= \\gamma^1 \\, ||V_0 - V^*||_\\infty .\n",
    "$$\n",
    "\n",
    "**Inductive hypothesis.** Assume for some $k\\ge 1$ that\n",
    "$$\n",
    "||V_k - V^*||_\\infty \\le \\gamma^{k}\\,||V_0 - V^*||_\\infty .\n",
    "$$\n",
    "\n",
    "**Inductive step.**\n",
    "$$\n",
    "||V_{k+1} - V^*||_\\infty\n",
    "= ||T^* V_k - T^* V^*||_\\infty\n",
    "\\le \\gamma \\, ||V_k - V^*||_\\infty\n",
    "\\le \\gamma^{k+1}\\,||V_0 - V^*||_\\infty .\n",
    "$$\n",
    "\n",
    "Thus, by induction,\n",
    "$$\n",
    "||V_k - V^*||_\\infty \\le \\gamma^{\\,k}\\,||V_0 - V^*||_\\infty \\quad \\text{for all } k\\ge 0.\n",
    "$$\n",
    "QED."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd797a5",
   "metadata": {},
   "source": [
    "### 2.3 Practical stopping rule\n",
    "\n",
    "In practice we never know what is the true $V^\\star$. But what we can calculate is the difference between two iterations. Here we (1) prove an error bound of $\\|V-V^\\star\\|_\\infty$ by $\\|V_{k+1} - V_k\\|_\\infty$:\n",
    "\n",
    "$$\n",
    "\\|V_k-V^\\star\\|_\\infty \\leq \\frac{\\|V_{k+1} - V_k\\|_\\infty}{1-\\gamma}\n",
    "$$\n",
    "\n",
    "and (2) Compute the tolerance on the consecutive-iterate gap $\\|V_{k+1}-V_k\\|_\\infty$ needed to guarantee $\\|V - V^\\star\\|_\\infty \\le 10^{-6}$ when $\\gamma=0.99$.\n",
    "\n",
    "**(TODO) Answer:**\n",
    "We start with:\n",
    "$$V_k-V^\\star=(V_k-V_{k+1})+(V_{k+1}-V^\\star)$$\n",
    "Applying the sup norm over all states $s\\in S$, we get:\n",
    "$$||V_k-V^\\star||_\\infty=||(V_k-V_{k+1})+(V_{k+1}-V^\\star)||_\\infty$$\n",
    "Applying the triangle inequality, we get:\n",
    "$$||V_k-V^\\star||_\\infty=||(V_k-V_{k+1})+(V_{k+1}-V^\\star)||_\\infty\\leq||(V_k-V_{k+1})||_\\infty+||(V_{k+1}-V^\\star)||_\\infty$$\n",
    "Applying the gamma contraction to the second term on the RHS, we get:\n",
    "$$||V_{k+1}-V^\\star||_\\infty \\leq \\gamma ||V_k-V^\\star||_\\infty$$\n",
    "Since inequalities are preserved under addition, after subbing in this result into the triangle inequality result, we get that:\n",
    "$$||V_k-V^\\star||_\\infty=||(V_k-V_{k+1})+(V_{k+1}-V^\\star)||_\\infty\\leq||(V_k-V_{k+1})||_\\infty+\\gamma ||V_k-V^\\star||_\\infty$$\n",
    "$$\\to ||V_k-V^\\star||_\\infty\\leq||(V_k-V_{k+1})||_\\infty+\\gamma ||V_k-V^\\star||_\\infty$$\n",
    "$$\\to ||V_k-V^\\star||_\\infty-\\gamma ||V_k-V^\\star||_\\infty\\leq||(V_k-V_{k+1})||_\\infty$$\n",
    "$$\\to (1-\\gamma)||V_k-V^\\star||_\\infty\\leq||(V_k-V_{k+1})||_\\infty$$\n",
    "Thus, we finally get that:\n",
    "$$\n",
    "\\boxed{\\|V_k-V^\\star\\|_\\infty \\leq \\frac{\\|V_{k+1} - V_k\\|_\\infty}{1-\\gamma}}\n",
    "$$\n",
    "since the sup norm is symmetric and because $1-\\gamma>0$, preserving the inequality direction.\n",
    "\n",
    "We now do the second part. To guarantee $\\|V_k - V^\\star\\|_\\infty \\le 10^{-6}$, it suffices to enforce $||V_{k+1}-V_k||_\\infty\\leq(1-\\gamma)\\cdot 10^{-6}$. Having $\\gamma=0.99$ and a tolerance of $10^{-6}$ means that we have:\n",
    "$$\\boxed{\\|V_{k+1} - V_k\\|_\\infty\\leq10^{-8}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a065e0a",
   "metadata": {},
   "source": [
    "### 2.4 The bound of iterations\n",
    "\n",
    "Assume $\\|V_1 - V_0\\|_\\infty = 1$, $\\gamma = 0.99$. How much iterations do we need to have $\\|V_k - V^\\star\\|_\\infty \\leq 10^{-6}$?\n",
    "\n",
    "**(TODO) Answer:**\n",
    "From 2.2, we get that $\\|V_k - V^\\star\\|_\\infty \\leq\\gamma^k||V_0-V^\\star||_\\infty$\n",
    "Then it suffices to prove $\\gamma^k||V_0-V^\\star||_\\infty\\leq10^{-6}$\n",
    "We also have from 2.3 that:\n",
    "$$||V_0-V^\\star||_\\infty\\leq\\frac{||V_1-V_0||_\\infty}{1-\\gamma}=\\frac{1}{0.01}=100$$\n",
    "Then, it suffices to prove that $\\gamma^k\\cdot 100\\leq10^{-6}$ because we found the upper bound of $||V_0-V^\\star||_\\infty$. We then have:\n",
    "$$0.99^k=10^{-8}$$\n",
    "Taking logorithms, we have that:\n",
    "$$k\\log(0.99)\\leq\\log(10^{-8})$$\n",
    "Since $log(0.99)<0$, we flip the inequality and get:\n",
    "$$\\boxed{k\\geq1833 \\text{ iterations}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd169530",
   "metadata": {},
   "source": [
    "## 3. Cliffwalk\n",
    "\n",
    "Implement policy evaluation, policy improvement, value iteration, and policy iteration for the `CliffWalking` task. For clarity and reproducibility, We include a minimal reimplementation of the environment that mirrors Gymnasium’s dynamics and reward scheme.\n",
    "\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/ComputationalRobotics/2025-ES-AM-158-PSET/main/PSET1/cliffwalk.png\" width=\"600\" alt=\"Inscribed polygon\">\n",
    "  <figcaption style=\"color:#6a737d; font-style:italic;\">\n",
    "    Figure 3. Illustration to cliffwalk problem.\n",
    "  </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782a72c",
   "metadata": {},
   "source": [
    "**CliffWalking (Gym-compatible) — Specification**\n",
    "\n",
    "- **Grid:** 4 rows × 12 columns (row-major indexing; `state_id = row * 12 + col`; index origin at top-left in comments).\n",
    "- **Start:** bottom-left cell `(row=3, col=0)`.\n",
    "- **Goal:** bottom-right cell `(row=3, col=11)`.\n",
    "- **Actions (4):** up (0), right (1), down (2), left (3).\n",
    "- **Rewards:** −1 per step; −100 on entering a cliff cell; 0 at the goal.\n",
    "- **Termination:** episode ends upon reaching the goal; this states are terminal/absorbing. If reaching cliff will go back to start.\n",
    "\n",
    "**Transition table**\n",
    "\n",
    "- `P[state][action] → list[(prob, next_state, reward, done)]`\n",
    "- Deterministic dynamics: each list contains a single tuple with `prob = 1.0` after handling boundaries, cliff, and goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3422e418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State count=48, Action count=4\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# Create Gym CliffWalking environment (v1).\n",
    "env_gym = gym.make(\"CliffWalking-v1\", render_mode=\"ansi\")\n",
    "nS, nA = env_gym.observation_space.n, env_gym.action_space.n\n",
    "# The CliffWalking grid is 4 × 12; actions are 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT.\n",
    "print(f\"State count={nS}, Action count={nA}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Utility: pretty-print a value function as a 2D grid (nrow × ncol).\n",
    "# Values can be any (nS,) array-like; states are indexed row-major:\n",
    "# State_id = row * ncol + col\n",
    "# -------------------------------------------------------------------\n",
    "def print_values(values, nrow: int, ncol: int, title: str = \"State Values\"):\n",
    "    \"\"\"Print a value table in grid form.\"\"\"\n",
    "    values = np.asarray(values).reshape(nrow, ncol)\n",
    "    print(title)\n",
    "    for r in range(nrow):\n",
    "        print(\" \".join(f\"{values[r, c]:6.2f}\" for c in range(ncol)))\n",
    "    print()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Utility: pretty-print a policy on the CliffWalking grid.\n",
    "# \n",
    "# Accepted pi formats for each state s:\n",
    "# - Int a               : deterministic action\n",
    "# - Length-4 vector     : Q-values or preferences; we render argmax (ties shown)\n",
    "# - Length-4 probabilities (stochastic policy): greedy action(s) by max prob\n",
    "# \n",
    "# Notes:\n",
    "# - Uses Gym's action order: 0=UP(↑), 1=RIGHT(→), 2=DOWN(↓), 3=LEFT(←)\n",
    "# - Terminal states in CliffWalking (bottom row except col=0) are marked:\n",
    "# S at (last_row, 0), C for cliff cells (last_row, 1..ncol-2), G at (last_row, ncol-1)\n",
    "# -------------------------------------------------------------------\n",
    "def print_policy(pi, nrow: int, ncol: int, title: str = \"Policy\"):\n",
    "    \"\"\"Print a deterministic/stochastic policy.\n",
    "    - If pi is a list of lists (length 4): treat as stochastic over [up, down, left, right].\n",
    "    - We render the greedy direction; if ties exist, we list all best arrows.\n",
    "    \"\"\"\n",
    "    arrow = {0:\"^\", 1:\">\", 2:\"v\", 3:\"<\"}  # Order aligned with env actions in this notebook\n",
    "    print(title)\n",
    "    for i in range(nrow):\n",
    "        row_syms = []\n",
    "        for j in range(ncol):\n",
    "            s = i*ncol + j\n",
    "            p = pi[s]\n",
    "            # Determine best action(s)\n",
    "            if isinstance(p, list) and len(p) == 4:\n",
    "                best = np.argwhere(np.array(p) == np.max(p)).flatten().tolist()\n",
    "            elif isinstance(p, int):\n",
    "                best = [p]\n",
    "            else:\n",
    "                # Fallback: greedy over provided vector/array\n",
    "                arr = np.array(p, dtype=float).ravel()\n",
    "                best = np.argwhere(arr == np.max(arr)).flatten().tolist()\n",
    "            # Special case: terminals on bottom row except j==0\n",
    "            if i == nrow-1 and j > 0:\n",
    "                row_syms.append(\"T\")\n",
    "            else:\n",
    "                row_syms.append(\"\".join(arrow[a] for a in best))\n",
    "        print(\" \".join(sym if sym else \".\" for sym in row_syms))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f7f98",
   "metadata": {},
   "source": [
    "### 3.1 Define Environment Model (no need to fill in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48012a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalkingEnv:\n",
    "    \"\"\"Cliff Walking environment (Gym-compatible dynamics)\n",
    "\n",
    "    State indexing\n",
    "    --------------\n",
    "    - Flattened row-major: state_id = row * ncol + col\n",
    "    - Rows: 0..nrow-1 (top → bottom), Cols: 0..ncol-1 (left → right)\n",
    "\n",
    "    Actions (match Gym/toy_text)\n",
    "    ----------------------------\n",
    "    - 0: UP (↑), 1: RIGHT (→), 2: DOWN (↓), 3: LEFT (←)\n",
    "\n",
    "    Grid (nrow=4, ncol=12)\n",
    "    ----------------------\n",
    "        [  0] [  1] [  2] [  3] [  4] [  5] [  6] [  7] [  8] [  9] [ 10] [ 11]\n",
    "        [ 12] [ 13] [ 14] [ 15] [ 16] [ 17] [ 18] [ 19] [ 20] [ 21] [ 22] [ 23]\n",
    "        [ 24] [ 25] [ 26] [ 27] [ 28] [ 29] [ 30] [ 31] [ 32] [ 33] [ 34] [ 35]\n",
    "        [36=S] [37=C] [38=C] [39=C] [40=C] [41=C] [42=C] [43=C] [44=C] [45=C] [46=C] [47=G]\n",
    "\n",
    "    Legend\n",
    "    ------\n",
    "    - S (start):  (row=3, col=0)   -> state 36\n",
    "    - C (cliff):  (row=3, col=1..10) -> states 37..46\n",
    "    - G (goal):   (row=3, col=11)  -> state 47\n",
    "\n",
    "    Termination & rewards\n",
    "    ---------------------\n",
    "    - Stepping into a cliff cell: reward = -100, done = False, go back to start\n",
    "    - Any other move:             reward = -1,   done = False\n",
    "    - Terminal states are absorbing: once in {goal}, any action keeps you there with reward 0.\n",
    "    \"\"\"\n",
    "\n",
    "    # Action constants for clarity\n",
    "    A_UP, A_RIGHT, A_DOWN, A_LEFT = 0, 1, 2, 3\n",
    "\n",
    "    def __init__(self, ncol: int = 12, nrow: int = 4):\n",
    "        self.ncol = int(ncol)\n",
    "        self.nrow = int(nrow)\n",
    "        self.nS = self.nrow * self.ncol\n",
    "        self.nA = 4\n",
    "        # Transition table: P[state][action] = [(prob, next_state, reward, done)]\n",
    "        self.P = self._create_P()\n",
    "\n",
    "    def _create_P(self):\n",
    "    # Allocate empty transition table\n",
    "        P = [[[] for _ in range(self.nA)] for _ in range(self.nS)]\n",
    "\n",
    "        # Movement deltas in (dx, dy), matching action order: 0↑, 1→, 2↓, 3←\n",
    "        # NOTE: x increases to the right (columns), y increases downward (rows).\n",
    "        deltas = {\n",
    "            self.A_UP:    ( 0, -1),\n",
    "            self.A_RIGHT: ( 1,  0),  # (1, 0) Written to hint order; same as (1, 0)\n",
    "            self.A_DOWN:  ( 0,  1),\n",
    "            self.A_LEFT:  (-1,  0),\n",
    "        }\n",
    "\n",
    "        start_s = (self.nrow - 1) * self.ncol + 0\n",
    "        goal_s  = (self.nrow - 1) * self.ncol + (self.ncol - 1)\n",
    "\n",
    "        for r in range(self.nrow):\n",
    "            for c in range(self.ncol):\n",
    "                s = r * self.ncol + c\n",
    "\n",
    "                if r == self.nrow - 1 and c > 0:\n",
    "                    for a in range(self.nA):\n",
    "                        P[s][a] = [(1.0, s, 0.0, True)]\n",
    "                    continue\n",
    "\n",
    "                for a in range(self.nA):\n",
    "                    dx, dy = deltas[a]\n",
    "\n",
    "                    nc = min(self.ncol - 1, max(0, c + dx))\n",
    "                    nr = min(self.nrow - 1, max(0, r + dy))\n",
    "\n",
    "                    ns = nr * self.ncol + nc\n",
    "                    reward = -1.0\n",
    "                    done = False\n",
    "\n",
    "                    if nr == self.nrow - 1 and 1 <= nc <= self.ncol - 2:\n",
    "                        ns = start_s          \n",
    "                        reward = -100.0\n",
    "                        done = False\n",
    "\n",
    "                    elif nr == self.nrow - 1 and nc == self.ncol - 1:\n",
    "                        done = True\n",
    "\n",
    "                    P[s][a] = [(1.0, ns, reward, done)]\n",
    "\n",
    "        return P\n",
    "\n",
    "\n",
    "# Build environment\n",
    "env = CliffWalkingEnv(ncol=12, nrow=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572fedfe",
   "metadata": {},
   "source": [
    "### 3.2 Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72beca54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluation converged in 494 iteration(s).\n",
      "Value Function under Random Policy\n",
      "-143.21 -147.36 -151.35 -153.93 -155.11 -155.06 -153.67 -150.46 -144.47 -134.46 -119.99 -105.22\n",
      "-164.99 -174.34 -180.41 -183.52 -184.80 -184.83 -183.63 -180.70 -174.70 -163.02 -141.34 -108.39\n",
      "-207.96 -237.09 -246.20 -249.36 -250.43 -250.52 -249.79 -247.82 -243.20 -231.68 -199.50 -97.21\n",
      "-261.35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(env, pi, gamma=0.95, theta=1e-10):\n",
    "    \"\"\"Iterative policy evaluation for a given stochastic policy π(a|s).\n",
    "\n",
    "    Args:\n",
    "        env: environment with a tabular transition model env.P where\n",
    "             P[s][a] = [(prob, next_state, reward, done)].\n",
    "        pi:  policy probabilities shaped [nS][4]; each pi[s] is a length-4 list\n",
    "             over actions [UP, RIGHT, DOWN, LEFT].\n",
    "        gamma: discount factor ∈ [0, 1).\n",
    "        theta: convergence threshold on the ∞-norm of value updates.\n",
    "\n",
    "    Returns:\n",
    "        v: list of state values of length nS.\n",
    "    \"\"\"\n",
    "    nS = env.nrow * env.ncol\n",
    "    v = [0.0] * nS  # Initialize V(s)=0\n",
    "    it = 1  # Iteration counter (logging only)\n",
    "\n",
    "    while True:\n",
    "        max_diff = 0.0\n",
    "        new_v = [0.0] * nS\n",
    "\n",
    "        for s in range(nS):\n",
    "            v_sum = 0.0  # Σ_a π(a|s) * Q(s,a)\n",
    "            for a in range(4):\n",
    "                #################################\n",
    "                # TODO: implement policy evaluation here\n",
    "                #################################\n",
    "                for (p, next_state, r, done) in env.P[s][a]:\n",
    "                    v_sum += p * (r + gamma * v[next_state])\n",
    "\n",
    "            new_v[s] = v_sum\n",
    "            max_diff = max(max_diff, abs(new_v[s] - v[s]))\n",
    "\n",
    "        v = new_v\n",
    "        if max_diff < theta:\n",
    "            break\n",
    "        it += 1\n",
    "\n",
    "    print(f\"Policy evaluation converged in {it} iteration(s).\")\n",
    "    return v\n",
    "\n",
    "\n",
    "# --- Example: evaluate a uniform random policy ---\n",
    "pi = [[0.25, 0.25, 0.25, 0.25] for _ in range(env.nrow * env.ncol)]\n",
    "gamma = 0.95\n",
    "\n",
    "v = policy_evaluation(env, pi, gamma)\n",
    "\n",
    "# Pretty-print the value function as a 4×12 grid\n",
    "print_values(v, env.nrow, env.ncol, title=\"Value Function under Random Policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da66ff59",
   "metadata": {},
   "source": [
    "### 3.3 Policy Iteration\n",
    "\n",
    "Policy Iteration alternates between:\n",
    "1) **Policy Evaluation**: compute the state-value function $V^{\\pi}$ of the current policy $\\pi$\n",
    "2) **Policy Improvement**: update $\\pi$ to be greedy w.r.t. $V^{\\pi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e91266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, pi, v, gamma=0.95):\n",
    "    \"\"\"Greedy policy improvement w.r.t. the current state-value function V.\n",
    "\n",
    "    For each state s:\n",
    "      1) Compute Q(s,a) = Σ_{s'} P(s'|s,a)[ r(s,a,s') + γ V(s') ] for all a.\n",
    "      2) Find the action(s) with maximal Q(s,a).\n",
    "      3) Update π(·|s) to split probability uniformly among all maximizers (tie-aware).\n",
    "\n",
    "    Args:\n",
    "        env: Tabular environment with transitions env.P where\n",
    "             P[s][a] = [(prob, next_state, reward, done)].\n",
    "        pi:  Current (possibly stochastic) policy, shape [nS][4]; updated in-place.\n",
    "        v:   Current state-value function V(s), length nS.\n",
    "        gamma: Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        pi: The improved policy (same object, updated in-place).\n",
    "    \"\"\"\n",
    "    nS = env.nrow * env.ncol\n",
    "    nA = 4\n",
    "    eps = 1e-8  # Numerical tolerance for tie-breaking\n",
    "\n",
    "    for s in range(nS):\n",
    "        q_list = []\n",
    "        for a in range(nA):\n",
    "            #################################\n",
    "            # TODO: implement policy iteration here\n",
    "            #################################\n",
    "\n",
    "        pi[s] = \n",
    "\n",
    "    print(\"Policy improvement completed.\")\n",
    "    return pi\n",
    "\n",
    "\n",
    "# --- Policy Iteration loop ---\n",
    "pi = [[0.25, 0.25, 0.25, 0.25] for _ in range(env.nrow * env.ncol)]\n",
    "iters = 0\n",
    "while True:\n",
    "    v = # TODO: add policy evaluation\n",
    "    old_pi = copy.deepcopy(pi)\n",
    "    new_pi = # TODO: add policy improvement\n",
    "    iters += 1\n",
    "    if old_pi == new_pi:  # Policy is stable\n",
    "        print(f\"Policy iteration converged in {iters} improvement step(s).\")\n",
    "        break\n",
    "\n",
    "# Report results\n",
    "print_values(v, env.nrow, env.ncol, title=\"Optimal Value Function\")\n",
    "print_policy(pi, env.nrow, env.ncol, title=\"Optimal Policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b7f8d",
   "metadata": {},
   "source": [
    "### 3.4 Value Iteration\n",
    "\n",
    "Value Iteration applies **Bellman optimality** updates directly to $V$. Or one can treat value iteration as one step policy evaluation plus one step policy improvement.\n",
    "\n",
    "After convergence, extract the greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate(env, gamma=0.95, theta=1e-10):\n",
    "    \"\"\"Value Iteration.\n",
    "\n",
    "    Updates V(s) <- max_a Σ_{s'} P(s'|s,a) [ r(s,a,s') + γ V(s') ]\n",
    "    until the maximum state-wise change is below `theta`.\n",
    "\n",
    "    Args:\n",
    "        env: Tabular environment exposing env.P with\n",
    "             P[s][a] = [(prob, next_state, reward, done)] and grid sizes nrow, ncol.\n",
    "        gamma (float): Discount factor in [0, 1).\n",
    "        theta (float): Convergence threshold on the infinity-norm of value updates.\n",
    "\n",
    "    Returns:\n",
    "        list[float]: The converged state-value function V of length nS (= nrow * ncol).\n",
    "\n",
    "    Notes:\n",
    "        - Terminal states are modeled as absorbing with reward 0 in `env.P`.\n",
    "          The Bellman backup naturally yields V(terminal) = 0.\n",
    "        - `deltas` (max per-iteration change) is tracked for debugging but not returned.\n",
    "    \"\"\"\n",
    "    nS, nA = env.nrow * env.ncol, 4\n",
    "    deltas = []\n",
    "    iters = 0\n",
    "    v = [0.0] * nS\n",
    "\n",
    "    while True:\n",
    "        iters += 1\n",
    "        max_diff = 0.0\n",
    "        new_v = [0.0] * nS\n",
    "\n",
    "        for s in range(nS):\n",
    "            # Bellman optimality backup: V(s) = max_a Q(s,a)\n",
    "            q_list = []\n",
    "            for a in range(nA):\n",
    "                #################################\n",
    "                # TODO: implement policy evaluation here\n",
    "                #################################\n",
    "            max_diff = max(max_diff, abs(new_v[s] - v[s]))\n",
    "\n",
    "        v = new_v\n",
    "        deltas.append(max_diff)\n",
    "        if max_diff < theta:\n",
    "            break\n",
    "    print(iters)\n",
    "    return v\n",
    "\n",
    "\n",
    "def greedy_policy(env, v, gamma=0.95):\n",
    "    \"\"\"Extract a greedy (tie-aware) policy from a value function.\n",
    "\n",
    "    For each state s, compute Q(s,a) and set π(a|s)=1/k for all actions a that\n",
    "    achieve the maximal Q-value (ties split uniformly); 0 otherwise.\n",
    "\n",
    "    Args:\n",
    "        env: Tabular environment with env.P.\n",
    "        v (list[float]): State-value function V(s).\n",
    "        gamma (float): Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        list[list[float]]: Policy π of shape [nS][4], each row summing to 1.\n",
    "    \"\"\"\n",
    "    nS, nA = env.nrow * env.ncol, 4\n",
    "    pi = [[0.0] * nA for _ in range(nS)]\n",
    "    eps = 1e-8  # Numerical tolerance for tie detection\n",
    "\n",
    "    for s in range(nS):\n",
    "        q_list = []\n",
    "        for a in range(nA):\n",
    "            q = 0.0\n",
    "            for (p, next_state, r, done) in env.P[s][a]:\n",
    "                q += p * (r if done else r + gamma * v[next_state])\n",
    "            q_list.append(q)\n",
    "\n",
    "        q_list = np.array(q_list, dtype=float)\n",
    "        max_q = q_list.max()\n",
    "        # Tie-aware argmax\n",
    "        opt_u = np.isclose(q_list, max_q, rtol=0.0, atol=eps)\n",
    "        k = int(opt_u.sum())\n",
    "        pi[s] = (opt_u / k).astype(float).tolist()\n",
    "\n",
    "    return pi\n",
    "\n",
    "\n",
    "# ----- Run Value Iteration and extract greedy policy -----\n",
    "gamma = 0.95  # Discount factor\n",
    "v = iterate(env, gamma=gamma)  # Assumes `env` is already constructed\n",
    "pi = greedy_policy(env, v, gamma=gamma)\n",
    "\n",
    "# Pretty-print results (assumes `print_values` and `print_policy` are defined)\n",
    "print_values(v, env.nrow, env.ncol, title=\"Optimal Value Function (Value Iteration)\")\n",
    "print_policy(pi, env.nrow, env.ncol, title=\"Optimal Policy (Value Iteration)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e865218",
   "metadata": {},
   "source": [
    "## 4. Matrix–vector Representation of DP\n",
    "\n",
    "We’ll use a small finite MDP ($|X|=16$, $|A|=4$) and **matrix** forms to compute the optimal $Q$-function:\n",
    "\n",
    "4.1. Build the **transition matrix**\n",
    "   $\n",
    "   P \\in \\mathbb{R}^{|X||A|\\times |X|}\n",
    "   $\n",
    "   and the **immediate reward** vector\n",
    "   $\n",
    "   r \\in \\mathbb{R}^{|X||A|}.\n",
    "   $\n",
    "\n",
    "4.2. Using the matrix form of the $Q$-value function $Q_\\pi$ and the value function $V_\\pi$ to write down the bellman equation.\n",
    "\n",
    "4.3. Define the **Bellman optimality operator**:\n",
    "   $$\n",
    "   T^\\star(Q) = r + \\gamma\\, P\\, J_Q,\n",
    "   $$\n",
    "   where\n",
    "   $$\n",
    "   (J_Q)(x) = \\max_{a} Q(x,a).\n",
    "   $$\n",
    "   Iterating $Q_{k+1} = T^\\star(Q_k)$ converges to the optimal $Q^\\star$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35440cec",
   "metadata": {},
   "source": [
    "### 4×4 Gridworld — From Bottom‑Left (Start) to Top‑Right (Goal)\n",
    "\n",
    "**States:** 16 cells in a 4×4 grid, row-major indexing with top-left as (row=0, col=0).\n",
    "State id: `s = row * 4 + col`, rows increase downward.\n",
    "\n",
    "**Start:** bottom-left `(row=3, col=0)` → `s_start = 12`\n",
    "**Goal:** top-right `(row=0, col=3)` → `s_goal = 3`\n",
    "\n",
    "**Actions (4):**\n",
    "- `a=0` → UP (↑)\n",
    "- `a=1` → RIGHT (→)\n",
    "- `a=2` → DOWN (↓)\n",
    "- `a=3` → LEFT (←)\n",
    "\n",
    "**Dynamics:** Deterministic. If an action would leave the grid world, the agent stays in place.\n",
    "\n",
    "**Rewards (maximize):**\n",
    "- `-1` per step\n",
    "- `0` in the goal\n",
    "\n",
    "**Terminal:** The goal is absorbing (from goal, any action keeps you at goal with reward 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ef97dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid 4×4. Start=S (row=3,col=0), Goal=G (row=0,col=3)\n",
      " 0  1  2  G \n",
      " 4  5  6  7\n",
      " 8  9 10 11\n",
      " S  13 14 15\n"
     ]
    }
   ],
   "source": [
    "# Grid size\n",
    "nrow, ncol = 4, 4\n",
    "nS = nrow * ncol  # |X| = 16\n",
    "nA = 4  # |A| = 4 (UP, RIGHT, DOWN, LEFT)\n",
    "\n",
    "# Start (bottom-left) and Goal (top-right)\n",
    "s_start = (nrow - 1) * ncol + 0  # 12\n",
    "s_goal  = 0 * ncol + (ncol - 1)  # 3\n",
    "\n",
    "# Row-major state id\n",
    "def s_id(r, c):\n",
    "    return r * ncol + c\n",
    "\n",
    "# For state-action row index in matrices of shape (nS*nA, ...)\n",
    "def sa_id(s, a):\n",
    "    return s * nA + a\n",
    "\n",
    "# Action deltas: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "DELTAS = {\n",
    "    0: (-1,  0),  # UP:    row-1\n",
    "    1: ( 0,  1),  # RIGHT: col+1\n",
    "    2: ( 1,  0),  # DOWN:  row+1\n",
    "    3: ( 0, -1),  # LEFT:  col-1\n",
    "}\n",
    "\n",
    "# Quick sanity checks and a tiny ASCII map\n",
    "print(\"Grid 4×4. Start=S (row=3,col=0), Goal=G (row=0,col=3)\")\n",
    "for rrow in range(nrow):\n",
    "    line = []\n",
    "    for ccol in range(ncol):\n",
    "        s = s_id(rrow, ccol)\n",
    "        if s == s_start:\n",
    "            line.append(\" S \")\n",
    "        elif s == s_goal:\n",
    "            line.append(\" G \")\n",
    "        else:\n",
    "            line.append(f\"{s:2d}\")\n",
    "    print(\" \".join(line))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e2d4d6",
   "metadata": {},
   "source": [
    "### 4.1 Build Transition Matrix and Reward Vector\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "- **Transition matrix** $P \\in \\mathbb{R}^{|X||A|\\times |X|}$\n",
    "  Rows index state–action pairs $(x,a)$, columns index next states $x'$.\n",
    "  Entry:\n",
    "  $$\n",
    "  P[(x,a),\\,x'] \\;\\equiv\\; \\Pr\\{X_{t+1}=x' \\mid X_t=x,\\; A_t=a\\}.\n",
    "  $$\n",
    "  Row-wise normalization holds: $\\sum_{x'} P[(x,a),x'] = 1$ for every $(x,a)$.\n",
    "\n",
    "- **Reward vector** $r \\in \\mathbb{R}^{|X||A|}$ (reward maximization form)\n",
    "  Each entry is the one-step expected reward under $(x,a)$:\n",
    "  $$\n",
    "  r[(x,a)] \\;\\equiv\\; \\mathbb{E}\\big[\\,R_{t+1}\\mid X_t=x,\\; A_t=a\\,\\big].\n",
    "  $$\n",
    "\n",
    "**Indexing note.**\n",
    "A convenient index for $(x,a)$ is\n",
    "$$\n",
    "i = x\\,|A| + a\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86995a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build P (|X||A| × |X|) and r (|X||A|)\n",
    "P = np.zeros((nS * nA, nS), dtype=float)\n",
    "r = np.zeros(nS * nA, dtype=float)\n",
    "\n",
    "for s in range(nS):\n",
    "# This will give // and %\n",
    "    r0, c0 = divmod(s, ncol)\n",
    "\n",
    "    for a in range(nA):\n",
    "        _s_id = sa_id(s, a)\n",
    "\n",
    "        # Goal is absorbing with reward 0\n",
    "        if s == s_goal:\n",
    "            P[_s_id, s_goal] = # TODO ...\n",
    "            r[_s_id] = # TODO ...\n",
    "            continue\n",
    "\n",
    "        dr, dc = DELTAS[a]\n",
    "        rr = min(nrow - 1, max(0, r0 + dr))\n",
    "        cc = min(ncol - 1, max(0, c0 + dc))\n",
    "        s_next = s_id(rr, cc)\n",
    "\n",
    "        # Deterministic transition\n",
    "        P[_s_id, s_next] = # TODO ...\n",
    "\n",
    "        # Reward: -1 per step, 0 in goal (already handled above)\n",
    "        r[_s_id] = # TODO ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5a23e0",
   "metadata": {},
   "source": [
    "### 4.2 Matrix Form of Bellman Consistency and Bellman equation\n",
    "\n",
    "Q-evaluation when a fixed policy $\\pi$ is given:\n",
    "\n",
    "$$\n",
    "Q_\\pi(x,a) = r(x,a) + \\gamma \\,\\mathbb{E}_{x'\\sim P(\\cdot \\mid x,a)} \\, V_\\pi(x') \\tag{4.3(1)}\n",
    "$$\n",
    "\n",
    "The bellman equation:\n",
    "$$\n",
    "Q^\\star(x,a) \\;=\\; r(x,a) \\;+\\; \\gamma \\, \\mathbb{E}_{x' \\sim P(\\cdot \\mid x,a)}\n",
    "\\left\\{ \\max_{a' \\in A} Q^\\star(x',a') \\right\\},\n",
    "\\qquad \\forall (x,a) \\in X \\times A. \\tag{4.3(2)}\n",
    "$$\n",
    "\n",
    "where $Q^\\star$ is the optimal $Q$-value function. Similarly, let us define\n",
    "\n",
    "$$\n",
    "J_Q(x) = \\max_{a \\in A} Q(x,a).\n",
    "$$\n",
    "\n",
    "Question: How to write these equations (4.3(1))&(2) in matrix and operator form?\n",
    "\n",
    "Note: write the equation using $r , Q_\\pi, Q^\\star \\in \\mathbb{R}^{|X||A|}$, $V_\\pi \\in \\mathbb{R}^{|X|}$, $P \\in \\mathbb{R}^{|X||A|\\times |X|}$ and matrix operator $J_Q$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570aac90",
   "metadata": {},
   "source": [
    "**(TODO) Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a258d",
   "metadata": {},
   "source": [
    "### 4.3 Solve bellman equation.\n",
    "\n",
    "Note that $J_Q$ has dimension $|X|$. With these notations, the *Bellman optimality operator* is defined as\n",
    "\n",
    "$$\n",
    "T^\\star Q \\;=\\; g + \\gamma P J_Q,\n",
    "\\tag{2.27}\n",
    "$$\n",
    "\n",
    "which is nothing but a matrix representation of the right-hand side of Bellman equation.\n",
    "This allows us to concisely write the Bellman equation as\n",
    "\n",
    "$$\n",
    "Q = T^\\star Q.\n",
    "\\tag{2.28}\n",
    "$$\n",
    "\n",
    "One can do to solve this equation is through *fix-point iteration*:\n",
    "$$\n",
    "Q_{n+1} = T^\\star Q_n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b44ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((nS * nA), dtype=float)\n",
    "for i in range(1000):\n",
    "    old_Q = Q.copy()\n",
    "    J_Q =  # TODO: V(s) = max_a Q(s,a)\n",
    "    Q = # TODO ...\n",
    "    if np.max(np.abs(Q - old_Q)) < 1e-10:\n",
    "        print(f\"Converged in {i+1} iterations.\")\n",
    "        break\n",
    "J_Q = Q.reshape(nS, nA).max(axis=1)    \n",
    "print(\"\\nOptimal state values J_Q (V*) on the grid:\")\n",
    "for r0 in range(nrow):\n",
    "    row_vals = []\n",
    "    for c0 in range(ncol):\n",
    "        s = r0 * ncol + c0\n",
    "        if s == s_start:\n",
    "            row_vals.append(\" S \")\n",
    "        elif s == s_goal:\n",
    "            row_vals.append(\" G \")\n",
    "        else:\n",
    "            row_vals.append(f\"{J_Q[s]:6.2f}\")\n",
    "    print(\" \".join(row_vals))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025ocrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
